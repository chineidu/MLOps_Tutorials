{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "# Built-in libraries\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "# Black formatter (optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/05 13:53:49 WARN Utils: Your hostname, Chinedus-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.151 instead (on interface en0)\n",
      "23/07/05 13:53:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/05 13:53:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RDD_examples\")\n",
    "\n",
    "# Spark Context\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### `.map()`\n",
    "\n",
    "- Return a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "```Python\n",
    "fp = \"some/data/filepath\"\n",
    "rdd = sc.textFile(fp)\n",
    "# Return a new RDD by applying a function to each element of this RDD.\n",
    "result = rdd.map(lambda row: row[0]) # Select the 0-th index\n",
    "result = rdd.map(lambda row: row[1]) # Select the 1-st index\n",
    "```\n",
    "<br>\n",
    "\n",
    "### `.countByValue()`\n",
    "\n",
    "- Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "  \n",
    "```Python\n",
    "fp = \"some/data/filepath\"\n",
    "rdd = sc.textFile(fp)\n",
    "result = rdd.map(lambda row: row[0]) # Select the 0-th index\n",
    "# return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "result.countByValue()  # returns {'3': 27145, '1': 6110, '2': 11370, '4': 34174, '5': 21201}\n",
    "```\n",
    "<br>\n",
    "\n",
    "### `rdd.collect()`\n",
    "\n",
    "- Return a list that contains all of the elements in this RDD.\n",
    "\n",
    "<br>\n",
    "\n",
    "### `.mapValues()`\n",
    "\n",
    "- This transformation applies a function to the values of each key-value pair in an RDD. It also retains the original RDD's partitioning.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 1, Count: 6110\n",
      "Rating: 2, Count: 11370\n",
      "Rating: 3, Count: 27145\n",
      "Rating: 4, Count: 34174\n",
      "Rating: 5, Count: 21201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the file and return it as an RDD of strings\n",
    "fp = \"../data/ml-100k/u.data\"\n",
    "lines = sc.textFile(fp)\n",
    "\n",
    "# Extract the ratings which is the 3rd field (2nd index)\n",
    "# columns: ['user id', 'movie id', 'rating', 'timestamp']\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "\n",
    "# Groupby the keys. i.e. (count the values of each rating)\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sorted_results = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sorted_results.items():\n",
    "    print(f\"Rating: {key}, Count: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Modules\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    LongType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Data As An RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: DataFrame[user_id: bigint, movie_id: bigint, rating: bigint, timestamp: string]\n",
      "\n",
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|    196|     242|     3|881250949|\n",
      "|    186|     302|     3|891717742|\n",
      "|     22|     377|     1|878887116|\n",
      "|    244|      51|     2|880606923|\n",
      "|    166|     346|     1|886397596|\n",
      "|    298|     474|     4|884182806|\n",
      "|    115|     265|     2|881171488|\n",
      "|    253|     465|     5|891628467|\n",
      "|    305|     451|     3|886324817|\n",
      "|      6|      86|     3|883603013|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the file and return it as an RDD of strings\n",
    "fp = \"../data/ml-100k/u.data\"\n",
    "raw_file = spark.sparkContext.textFile(fp)\n",
    "\n",
    "\n",
    "def movie_mapper(row: str) -> Row:\n",
    "    \"\"\"this returns a Row object.\"\"\"\n",
    "    data = row.split()\n",
    "    # ['user id', 'movie id', 'rating', 'timestamp']\n",
    "    result = Row(\n",
    "        user_id=int(data[0]),\n",
    "        movie_id=int(data[1]),\n",
    "        rating=int(data[2]),\n",
    "        timestamp=data[3],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# Map the RDD\n",
    "rdd = raw_file.map(movie_mapper)\n",
    "\n",
    "# Convert the RDD to DataFrame and infer schema\n",
    "df = spark.createDataFrame(rdd).cache()\n",
    "print(f\"Schema: {df}\\n\")\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "df.createOrReplaceTempView(\"movies\")\n",
    "\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|    298|     474|     4|884182806|\n",
      "|    291|    1042|     4|874834944|\n",
      "|    119|     392|     4|886176814|\n",
      "|    167|     486|     4|892738452|\n",
      "|    299|     144|     4|877881320|\n",
      "|    308|       1|     4|887736532|\n",
      "|     63|     277|     4|875747401|\n",
      "|    301|      98|     4|882075827|\n",
      "|    225|     193|     4|879539727|\n",
      "|    290|      88|     4|880731963|\n",
      "|    157|     274|     4|886890835|\n",
      "|      7|      32|     4|891350932|\n",
      "|     10|      16|     4|877888877|\n",
      "|    284|     304|     4|885329322|\n",
      "|    251|     100|     4|886271884|\n",
      "|    260|     322|     4|890618898|\n",
      "|     87|     384|     4|879877127|\n",
      "|    292|     515|     4|881103977|\n",
      "|    201|     219|     4|884112673|\n",
      "|    246|     919|     4|884920949|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the movies with a rating of 4\n",
    "result = spark.sql(\"SELECT * FROM movies WHERE rating = 4;\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|rating|num_votes|\n",
      "+------+---------+\n",
      "|     1|     6110|\n",
      "|     2|    11370|\n",
      "|     3|    27145|\n",
      "|     4|    34174|\n",
      "|     5|    21201|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT rating, COUNT(*) AS num_votes \n",
    "        FROM movies \n",
    "    GROUP BY rating\n",
    "    ORDER BY rating ASC;\n",
    "\"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|    298|     474|     4|884182806|\n",
      "|    291|    1042|     4|874834944|\n",
      "|    119|     392|     4|886176814|\n",
      "|    167|     486|     4|892738452|\n",
      "|    299|     144|     4|877881320|\n",
      "|    308|       1|     4|887736532|\n",
      "|     63|     277|     4|875747401|\n",
      "|    301|      98|     4|882075827|\n",
      "|    225|     193|     4|879539727|\n",
      "|    290|      88|     4|880731963|\n",
      "|    157|     274|     4|886890835|\n",
      "|      7|      32|     4|891350932|\n",
      "|     10|      16|     4|877888877|\n",
      "|    284|     304|     4|885329322|\n",
      "|    251|     100|     4|886271884|\n",
      "|    260|     322|     4|890618898|\n",
      "|     87|     384|     4|879877127|\n",
      "|    292|     515|     4|881103977|\n",
      "|    201|     219|     4|884112673|\n",
      "|    246|     919|     4|884920949|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the movies with a rating of 4\n",
    "df.filter((df[\"rating\"] == 4)).show()\n",
    "\n",
    "# OR\n",
    "# df.filter(func.col(\"rating\") == 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'user_id'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a DF\n",
    "df.select(\"user_id\")\n",
    "\n",
    "# Returns a column\n",
    "df[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friends_mapper(row: str) -> Row:\n",
    "    \"\"\"This is used to map the data and it returns a Row object.\"\"\"\n",
    "    _data = row.split(\",\")\n",
    "    result = Row(\n",
    "        user_id=(_data[0]),\n",
    "        name=_data[1],\n",
    "        age=(_data[2]),\n",
    "        num_friends=(_data[3]),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---+-----------+\n",
      "|user_id|    name|age|num_friends|\n",
      "+-------+--------+---+-----------+\n",
      "| userID|    name|age|    friends|\n",
      "|      0|    Will| 33|        385|\n",
      "|      1|Jean-Luc| 26|          2|\n",
      "|      2|    Hugh| 55|        221|\n",
      "|      3|  Deanna| 40|        465|\n",
      "|      4|   Quark| 68|         21|\n",
      "|      5|  Weyoun| 59|        318|\n",
      "|      6|  Gowron| 37|        220|\n",
      "|      7|    Will| 54|        307|\n",
      "|      8|  Jadzia| 38|        380|\n",
      "|      9|    Hugh| 27|        181|\n",
      "|     10|     Odo| 53|        191|\n",
      "|     11|     Ben| 57|        372|\n",
      "|     12|   Keiko| 54|        253|\n",
      "|     13|Jean-Luc| 56|        444|\n",
      "|     14|    Hugh| 43|         49|\n",
      "|     15|     Rom| 36|         49|\n",
      "|     16|  Weyoun| 22|        323|\n",
      "|     17|     Odo| 35|         13|\n",
      "|     18|Jean-Luc| 45|        455|\n",
      "+-------+--------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data as an RDD\n",
    "fp = \"../data/fakefriends-header.csv\"\n",
    "data = spark.sparkContext.textFile(fp)\n",
    "\n",
    "# Map the RDD\n",
    "rdd = data.map(friends_mapper)\n",
    "# Load as DF and infer schema\n",
    "friends_df = spark.createDataFrame(rdd).cache()\n",
    "friends_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|age|avg_num_friends|\n",
      "+---+---------------+\n",
      "| 63|          384.0|\n",
      "| 21|         350.88|\n",
      "| 18|         343.38|\n",
      "| 52|         340.64|\n",
      "| 33|         325.33|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# friends_df.groupBy(func.col(\"age\"))\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "friends_df.createOrReplaceTempView(\"friends_df\")\n",
    "\n",
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT age, ROUND(AVG(num_friends), 2) as avg_num_friends\n",
    "        FROM friends_df\n",
    "    GROUP BY age\n",
    "    ORDER BY avg_num_friends DESC;\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data as a Spark DataFrame\n",
    "fp = \"../data/fakefriends-header.csv\"\n",
    "friends_df = spark.read.option(\"header\", \"true\").option(\"InferSchema\", \"true\").csv(fp)\n",
    "friends_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|age|avg_num_friends|\n",
      "+---+---------------+\n",
      "| 63|          384.0|\n",
      "| 21|         350.88|\n",
      "| 18|         343.38|\n",
      "| 52|         340.64|\n",
      "| 33|         325.33|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.groupBy([\"age\"]).agg(\n",
    "    func.round(func.avg(\"friends\"), 2).alias(\"avg_num_friends\")\n",
    ").sort(\"avg_num_friends\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pclass: integer (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- sibsp: integer (nullable = true)\n",
      " |-- parch: integer (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- cabin: string (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- body: integer (nullable = true)\n",
      " |-- home.dest: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp = \"../data/titanic_data.csv\"\n",
    "titanic_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .format(\"csv\")\n",
    "    .load(fp)\n",
    ")\n",
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-----+--------+----+----+--------------------+\n",
      "|pclass|survived|                name|   sex|   age|sibsp|parch|ticket|    fare|cabin|embarked|boat|body|           home.dest|\n",
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-----+--------+----+----+--------------------+\n",
      "|     1|       1|Allen, Miss. Elis...|female|  29.0|    0|    0| 24160|211.3375|   B5|       S|   2|null|        St Louis, MO|\n",
      "|     1|       1|Allison, Master. ...|  male|0.9167|    1|    2|113781|  151.55|  C22|       S|  11|null|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Miss. He...|female|   2.0|    1|    2|113781|  151.55|  C22|       S|null|null|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mr. Huds...|  male|  30.0|    1|    2|113781|  151.55|  C22|       S|null| 135|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mrs. Hud...|female|  25.0|    1|    2|113781|  151.55|  C22|       S|null|null|Montreal, PQ / Ch...|\n",
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-----+--------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "#### `func.explode()`\n",
    "\n",
    "- Returns a new row for each element in the given array or map.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func.udf(returnType=ArrayType(StringType()))\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Docs!\"\"\"\n",
    "    pattern = re.compile(r\"\\W+\")  # Select characters that are not words\n",
    "    # Split the text using the pattern and convert to lowercase\n",
    "    return pattern.split(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|normalized_words|                name|\n",
      "+----------------+--------------------+\n",
      "|           allen|Allen, Miss. Elis...|\n",
      "|            miss|Allen, Miss. Elis...|\n",
      "|       elisabeth|Allen, Miss. Elis...|\n",
      "|          walton|Allen, Miss. Elis...|\n",
      "|         allison|Allison, Master. ...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of characters\n",
    "words_flattened = titanic_df.select(\n",
    "    func.explode(normalize_text(\"name\")).alias(\"normalized_words\"), \"name\"\n",
    ")\n",
    "\n",
    "words_flattened.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|normalized_words|frequency|\n",
      "+----------------+---------+\n",
      "|              mr|      763|\n",
      "|                |      263|\n",
      "|            miss|      260|\n",
      "|             mrs|      201|\n",
      "|         william|       87|\n",
      "+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_flattened.groupBy([\"normalized_words\"]).agg(\n",
    "    func.count(\"normalized_words\").alias(\"frequency\")\n",
    ").sort(\"frequency\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|normalized_words|                name|\n",
      "+----------------+--------------------+\n",
      "|           allen|Allen, Miss. Elis...|\n",
      "|            miss|Allen, Miss. Elis...|\n",
      "|       elisabeth|Allen, Miss. Elis...|\n",
      "|          walton|Allen, Miss. Elis...|\n",
      "|         allison|Allison, Master. ...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = titanic_df.select(\n",
    "    func.explode(func.split(func.lower(func.col(\"name\")), pattern=r\"\\W+\")).alias(\n",
    "        \"normalized_words\"\n",
    "    ),\n",
    "    \"name\",\n",
    ")\n",
    "\n",
    "res.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|normalized_words|frequency|\n",
      "+----------------+---------+\n",
      "|              mr|      763|\n",
      "|                |      263|\n",
      "|            miss|      260|\n",
      "|             mrs|      201|\n",
      "|         william|       87|\n",
      "+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.groupBy([\"normalized_words\"]).agg(\n",
    "    func.count(\"normalized_words\").alias(\"frequency\")\n",
    ").sort(\"frequency\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-----+--------+----+----+--------------------+\n",
      "|pclass|survived|                name|   sex|   age|sibsp|parch|ticket|    fare|cabin|embarked|boat|body|           home.dest|\n",
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-----+--------+----+----+--------------------+\n",
      "|     1|       1|Allen, Miss. Elis...|female|  29.0|    0|    0| 24160|211.3375|   B5|       S|   2|null|        St Louis, MO|\n",
      "|     1|       1|Allison, Master. ...|  male|0.9167|    1|    2|113781|  151.55|  C22|       S|  11|null|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Miss. He...|female|   2.0|    1|    2|113781|  151.55|  C22|       S|null|null|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mr. Huds...|  male|  30.0|    1|    2|113781|  151.55|  C22|       S|null| 135|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mrs. Hud...|female|  25.0|    1|    2|113781|  151.55|  C22|       S|null|null|Montreal, PQ / Ch...|\n",
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-----+--------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+------+------+-----+-----+---------+------+-----+--------+----+----+--------------------+\n",
      "|pclass|survived|                name|   sex|   age|sibsp|parch|   ticket|  fare|cabin|embarked|boat|body|           home.dest|\n",
      "+------+--------+--------------------+------+------+-----+-----+---------+------+-----+--------+----+----+--------------------+\n",
      "|     3|       1|Dean, Miss. Eliza...|female|0.1667|    1|    2|C.A. 2315|20.575| null|       S|  10|null|Devon, England Wi...|\n",
      "+------+--------+--------------------+------+------+-----+-----+---------+------+-----+--------+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the youngest passenger\n",
    "min_temp = titanic_df.select(func.min(\"age\")).first()[0]\n",
    "\n",
    "titanic_df.filter(func.col(\"age\") == min_temp).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name|               LOWER|\n",
      "+--------------------+--------------------+\n",
      "|Allen, Miss. Elis...|allen, miss. elis...|\n",
      "|Allison, Master. ...|allison, master. ...|\n",
      "|Allison, Miss. He...|allison, miss. he...|\n",
      "|Allison, Mr. Huds...|allison, mr. huds...|\n",
      "|Allison, Mrs. Hud...|allison, mrs. hud...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column\n",
    "titanic_df.withColumn(\"LOWER\", func.lower(func.col(\"name\"))).select(\n",
    "    [\"name\", \"LOWER\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
