{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "# Built-in libraries\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "# Black formatter (optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/05 10:24:26 WARN Utils: Your hostname, Chinedus-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.151 instead (on interface en0)\n",
      "23/07/05 10:24:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/05 10:24:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RDD_examples\")\n",
    "\n",
    "# Spark Context\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### `.map()`\n",
    "\n",
    "- Return a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "```Python\n",
    "fp = \"some/data/filepath\"\n",
    "rdd = sc.textFile(fp)\n",
    "# Return a new RDD by applying a function to each element of this RDD.\n",
    "result = rdd.map(lambda row: row[0]) # Select the 0-th index\n",
    "result = rdd.map(lambda row: row[1]) # Select the 1-st index\n",
    "```\n",
    "<br>\n",
    "\n",
    "### `.countByValue()`\n",
    "\n",
    "- Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "  \n",
    "```Python\n",
    "fp = \"some/data/filepath\"\n",
    "rdd = sc.textFile(fp)\n",
    "result = rdd.map(lambda row: row[0]) # Select the 0-th index\n",
    "# return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "result.countByValue()  # returns {'3': 27145, '1': 6110, '2': 11370, '4': 34174, '5': 21201}\n",
    "```\n",
    "<br>\n",
    "\n",
    "### `rdd.collect()`\n",
    "\n",
    "- Return a list that contains all of the elements in this RDD.\n",
    "\n",
    "<br>\n",
    "\n",
    "### `.mapValues()`\n",
    "\n",
    "- This transformation applies a function to the values of each key-value pair in an RDD. It also retains the original RDD's partitioning.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 1, Count: 6110\n",
      "Rating: 2, Count: 11370\n",
      "Rating: 3, Count: 27145\n",
      "Rating: 4, Count: 34174\n",
      "Rating: 5, Count: 21201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the file and return it as an RDD of strings\n",
    "fp = \"../data/ml-100k/u.data\"\n",
    "lines = sc.textFile(fp)\n",
    "\n",
    "# Extract the ratings which is the 3rd field (2nd index)\n",
    "# columns: ['user id', 'movie id', 'rating', 'timestamp']\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "\n",
    "# Groupby the keys. i.e. (count the values of each rating)\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sorted_results = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sorted_results.items():\n",
    "    print(f\"Rating: {key}, Count: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Modules\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    LongType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: DataFrame[user_id: bigint, movie_id: bigint, rating: bigint, timestamp: string]\n",
      "\n",
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|    196|     242|     3|881250949|\n",
      "|    186|     302|     3|891717742|\n",
      "|     22|     377|     1|878887116|\n",
      "|    244|      51|     2|880606923|\n",
      "|    166|     346|     1|886397596|\n",
      "|    298|     474|     4|884182806|\n",
      "|    115|     265|     2|881171488|\n",
      "|    253|     465|     5|891628467|\n",
      "|    305|     451|     3|886324817|\n",
      "|      6|      86|     3|883603013|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the file and return it as an RDD of strings\n",
    "fp = \"../data/ml-100k/u.data\"\n",
    "raw_file = spark.sparkContext.textFile(fp)\n",
    "\n",
    "\n",
    "def movie_mapper(row: str) -> Row:\n",
    "    \"\"\"this returns a Row object.\"\"\"\n",
    "    data = row.split()\n",
    "    # ['user id', 'movie id', 'rating', 'timestamp']\n",
    "    result = Row(\n",
    "        user_id=int(data[0]),\n",
    "        movie_id=int(data[1]),\n",
    "        rating=int(data[2]),\n",
    "        timestamp=data[3],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# Map the RDD\n",
    "rdd = raw_file.map(movie_mapper)\n",
    "\n",
    "# Convert the RDD to DataFrame and infer schema\n",
    "schema_ = spark.createDataFrame(rdd).cache()\n",
    "print(f\"Schema: {schema_}\\n\")\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schema_.createOrReplaceTempView(\"movies\")\n",
    "\n",
    "\n",
    "schema_.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|    298|     474|     4|884182806|\n",
      "|    291|    1042|     4|874834944|\n",
      "|    119|     392|     4|886176814|\n",
      "|    167|     486|     4|892738452|\n",
      "|    299|     144|     4|877881320|\n",
      "|    308|       1|     4|887736532|\n",
      "|     63|     277|     4|875747401|\n",
      "|    301|      98|     4|882075827|\n",
      "|    225|     193|     4|879539727|\n",
      "|    290|      88|     4|880731963|\n",
      "|    157|     274|     4|886890835|\n",
      "|      7|      32|     4|891350932|\n",
      "|     10|      16|     4|877888877|\n",
      "|    284|     304|     4|885329322|\n",
      "|    251|     100|     4|886271884|\n",
      "|    260|     322|     4|890618898|\n",
      "|     87|     384|     4|879877127|\n",
      "|    292|     515|     4|881103977|\n",
      "|    201|     219|     4|884112673|\n",
      "|    246|     919|     4|884920949|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT * FROM movies WHERE rating = 4;\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|rating|num_votes|\n",
      "+------+---------+\n",
      "|     1|     6110|\n",
      "|     2|    11370|\n",
      "|     3|    27145|\n",
      "|     4|    34174|\n",
      "|     5|    21201|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT rating, COUNT(*) AS num_votes \n",
    "        FROM movies \n",
    "    GROUP BY rating\n",
    "    ORDER BY rating ASC;\n",
    "\"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../data/fakefriends-header.csv\"\n",
    "data = sc.textFile(fp)\n",
    "\n",
    "res = data.map(lambda row: (row.split(\",\")[2], row.split(\",\")[3]))  # age, num_friends\n",
    "\n",
    "\n",
    "result = res.mapValues(lambda row: (row, 1))\n",
    "# result.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).collect()\n",
    "result.reduceByKey(lambda x, y: (x[1] + y[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lines.map(lambda row: row.split()[1])\n",
    "res.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
